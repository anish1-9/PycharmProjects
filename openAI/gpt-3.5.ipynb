{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-31T18:12:22.301008300Z",
     "start_time": "2023-05-31T18:12:22.282000900Z"
    }
   },
   "outputs": [],
   "source": [
    "import time,re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pdfplumber, random, string\n",
    "from openai.error import RateLimitError\n",
    "import os\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "MAX_TOKENS = 4097\n",
    "OVERLAP = 200  # Number of tokens to overlap between chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def generate_random_string(length=10):\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for _ in range(length))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T18:12:22.338521600Z",
     "start_time": "2023-05-31T18:12:22.299011800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def extract_text(file_path):\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        text_list = []\n",
    "        for page in pdf.pages:\n",
    "            text_list.append(page.extract_text().lower())\n",
    "    return ' '.join(text_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T18:12:22.338521600Z",
     "start_time": "2023-05-31T18:12:22.311522100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def summarize_text(text):\n",
    "    summary_chunks=[]\n",
    "    return ' '.join(summary_chunks)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T18:12:22.354527300Z",
     "start_time": "2023-05-31T18:12:22.326520700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "import tinycss2.tokenizer\n",
    "\n",
    "\n",
    "class Session:\n",
    "    def __init__(self):\n",
    "        self.conversation_history = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        ]\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        self.conversation_history.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    def create_responses(self, chunk, prompt):\n",
    "        self.add_message(\"user\", chunk)\n",
    "        self.add_message(\"user\", prompt)\n",
    "\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=self.conversation_history[-(MAX_TOKENS//2):]  # Limit to the last part of conversation that fits in MAX_TOKENS\n",
    "            )\n",
    "            response_content = response['choices'][0]['message']['content'].strip()\n",
    "            self.add_message(\"assistant\", response_content)\n",
    "            return response_content\n",
    "        except RateLimitError:\n",
    "            print(\"Rate limit exceeded. Retrying after delay...\")\n",
    "            time.sleep(5)  # Delay for 5 seconds\n",
    "            return self.create_responses(chunk, prompt)  # Retry\n",
    "\n",
    "    def answer_prompt(self, text, prompt):\n",
    "        chunks = []\n",
    "        current_chunk = ''\n",
    "\n",
    "        # Split the text into words\n",
    "        words = text.split(' ')\n",
    "\n",
    "        for word in words:\n",
    "            # If adding the next word doesn't exceed the maximum length, add the word to the chunk\n",
    "            if len(tinycss2.tokenizer.encode(current_chunk + ' ' + word)) < MAX_TOKENS:\n",
    "                current_chunk += ' ' + word\n",
    "            else:\n",
    "                # If adding the next word would exceed the maximum length, save the current chunk and start a new one\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = word\n",
    "\n",
    "        # Make sure to add the last chunk if it's non-empty\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "\n",
    "        responses = []\n",
    "\n",
    "        # use ThreadPoolExecutor to process chunks in parallel\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            # create a tqdm progress bar\n",
    "            with tqdm(total=len(chunks), desc=\"Answering\", unit=\"chunk\") as pbar:\n",
    "                futures = [executor.submit(self.create_responses, chunk, prompt) for chunk in chunks]\n",
    "                for future in as_completed(futures):\n",
    "                    try:\n",
    "                        response = future.result()\n",
    "                        responses.append(response)\n",
    "                    except Exception as exc:\n",
    "                        print('A chunk generated an exception: %s' % exc)\n",
    "                    # increment the progress bar\n",
    "                    pbar.update()\n",
    "\n",
    "        # return the first response\n",
    "        return responses[0]\n",
    "\n",
    "    def interactive_session(self, file_path):\n",
    "        text = extract_text(file_path)\n",
    "        last_action = None  # variable to remember the last action\n",
    "\n",
    "        while True:\n",
    "            action = last_action if last_action else input(\n",
    "                \"Type 'ask' to ask a question, 'summarize' to summarize the PDF, or 'exit' to quit: \")\n",
    "\n",
    "            if action.lower() == 'exit':\n",
    "                break\n",
    "            elif action.lower() == 'ask':\n",
    "                prompt = input(\"Ask your question: \")\n",
    "                response = self.answer_prompt(text, prompt)\n",
    "                print(f\"AI's response: {response}\")\n",
    "\n",
    "                # Ask user for desired file type\n",
    "                file_type = input(\"Please enter the desired file type (txt, doc, csv, etc.): \")\n",
    "                filename = sanitize_filename(prompt) + '.' + file_type  # generate filename\n",
    "\n",
    "                # Save response to file\n",
    "                with open(f'output/{filename}', 'w') as f:\n",
    "                    f.write(f'Question: {prompt}\\nAI Response: {response}')\n",
    "\n",
    "                last_action = None  # Reset last_action\n",
    "            elif action.lower() == 'summarize':\n",
    "                start_time = time.time()  # Start timer\n",
    "                summary, time_taken = summarize_pdf(file_path)\n",
    "\n",
    "                # Save summary to file\n",
    "                with open('output_summary.txt', 'w',encoding='utf-8') as f:\n",
    "                    f.write(summary)\n",
    "\n",
    "                print(f\"Summarized text: {summary}\")\n",
    "                print(f\"Time taken: {time_taken:.2f} seconds or {time_taken / 60:.2f} minutes\")\n",
    "                last_action = None  # Reset last_action\n",
    "            else:\n",
    "                print(\"Invalid option. Please try again.\")\n",
    "                last_action = None  # Reset last_action"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T18:12:22.400528Z",
     "start_time": "2023-05-31T18:12:22.356529200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def sanitize_filename(prompt):\n",
    "            # Remove any characters that aren't alphanumeric, underscores, or hyphens\n",
    "    filename = re.sub(r'[^a-zA-Z0-9_-]', '', prompt.replace(' ', '_'))  # replace spaces with underscores\n",
    "    return filename\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T18:12:22.401526600Z",
     "start_time": "2023-05-31T18:12:22.383520Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def summarize_pdf(file_path):\n",
    "    text = extract_text(file_path)\n",
    "    start_time = time.time()  # Start timer\n",
    "    summary = summarize_text(text)\n",
    "    elapsed_time = time.time() - start_time  # Calculate elapsed time\n",
    "    return summary, elapsed_time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T18:12:22.418050900Z",
     "start_time": "2023-05-31T18:12:22.393521400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized text: \n",
      "Time taken: 0.00 seconds or 0.00 minutes\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tinycss2.tokenizer' has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[33], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m session \u001B[38;5;241m=\u001B[39m Session()\n\u001B[1;32m----> 2\u001B[0m \u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minteractive_session\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mC:\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mUsers\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mGF 63\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mPycharmProjects\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mpythonProject1\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mlemh207.pdf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[30], line 81\u001B[0m, in \u001B[0;36mSession.interactive_session\u001B[1;34m(self, file_path)\u001B[0m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m action\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mask\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m     80\u001B[0m     prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsk your question: \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 81\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43manswer_prompt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     82\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAI\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms response: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     84\u001B[0m     \u001B[38;5;66;03m# Ask user for desired file type\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[30], line 39\u001B[0m, in \u001B[0;36mSession.answer_prompt\u001B[1;34m(self, text, prompt)\u001B[0m\n\u001B[0;32m     35\u001B[0m words \u001B[38;5;241m=\u001B[39m text\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m words:\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;66;03m# If adding the next word doesn't exceed the maximum length, add the word to the chunk\u001B[39;00m\n\u001B[1;32m---> 39\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[43mtinycss2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m(current_chunk \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m word)) \u001B[38;5;241m<\u001B[39m MAX_TOKENS:\n\u001B[0;32m     40\u001B[0m         current_chunk \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m word\n\u001B[0;32m     41\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     42\u001B[0m         \u001B[38;5;66;03m# If adding the next word would exceed the maximum length, save the current chunk and start a new one\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'tinycss2.tokenizer' has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "session = Session()\n",
    "session.interactive_session(r\"C:\\Users\\GF 63\\PycharmProjects\\pythonProject1\\lemh207.pdf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T18:12:46.546542Z",
     "start_time": "2023-05-31T18:12:22.422051400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
